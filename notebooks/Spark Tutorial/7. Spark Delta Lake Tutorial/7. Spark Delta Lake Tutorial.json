{
  "paragraphs": [
    {
      "text": "%md\n\n# Introduction\n\nThis is a tutorial for using spark [delta lake](https://delta.io/) in Zeppelin. You need to run the following paragraph first to load delta package.\n\n",
      "user": "anonymous",
      "dateUpdated": "May 4, 2020, 2:11:57 PM",
      "progress": 0,
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h1>Introduction</h1>\n<p>This is a tutorial for using spark <a href=\"https://delta.io/\">delta lake</a> in Zeppelin. You need to run the following paragraph first to load delta package.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588572279774_1507831415",
      "id": "paragraph_1588572279774_1507831415",
      "dateCreated": "May 4, 2020, 2:04:39 PM",
      "dateStarted": "May 4, 2020, 2:11:57 PM",
      "dateFinished": "May 4, 2020, 2:11:58 PM",
      "status": "FINISHED"
    },
    {
      "text": "%spark.conf\n\nspark.jars.packages io.delta:delta-core_2.11:0.6.0",
      "user": "anonymous",
      "dateUpdated": "May 4, 2020, 2:12:12 PM",
      "progress": 0,
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/text"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588147206215_1200788867",
      "id": "paragraph_1588147206215_1200788867",
      "dateCreated": "Apr 29, 2020, 4:00:06 PM",
      "dateStarted": "Apr 29, 2020, 4:10:33 PM",
      "dateFinished": "Apr 29, 2020, 4:10:33 PM",
      "status": "FINISHED"
    },
    {
      "title": "Create a table",
      "text": "%spark\n\nval data = spark.range(0, 5)\ndata.write.format(\"delta\").save(\"/tmp/delta-table\")\n",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2020, 4:13:31 PM",
      "progress": 0,
      "config": {
        "colWidth": 6.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mdata\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[Long]\u001b[0m = [id: bigint]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588147833426_1914590471",
      "id": "paragraph_1588147833426_1914590471",
      "dateCreated": "Apr 29, 2020, 4:10:33 PM",
      "dateStarted": "Apr 29, 2020, 4:11:45 PM",
      "dateFinished": "Apr 29, 2020, 4:11:49 PM",
      "status": "FINISHED"
    },
    {
      "title": "Read a table",
      "text": "%spark\n\nval df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")\ndf.show()",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2020, 4:13:35 PM",
      "progress": 0,
      "config": {
        "colWidth": 6.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+---+\n| id|\n+---+\n|  0|\n|  3|\n|  1|\n|  2|\n|  4|\n+---+\n\n\u001b[1m\u001b[34mdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [id: bigint]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588147853461_1624743216",
      "id": "paragraph_1588147853461_1624743216",
      "dateCreated": "Apr 29, 2020, 4:10:53 PM",
      "dateStarted": "Apr 29, 2020, 4:11:55 PM",
      "dateFinished": "Apr 29, 2020, 4:11:56 PM",
      "status": "FINISHED"
    },
    {
      "title": "Overwrite",
      "text": "%spark\n\nval data = spark.range(5, 10)\ndata.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\")\ndf.show()",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2020, 4:14:41 PM",
      "progress": 0,
      "config": {
        "colWidth": 6.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+---+\n| id|\n+---+\n|  5|\n|  6|\n|  7|\n|  9|\n|  8|\n+---+\n\n\u001b[1m\u001b[34mdata\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[Long]\u001b[0m = [id: bigint]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588148062120_1790808564",
      "id": "paragraph_1588148062120_1790808564",
      "dateCreated": "Apr 29, 2020, 4:14:22 PM",
      "dateStarted": "Apr 29, 2020, 4:14:41 PM",
      "dateFinished": "Apr 29, 2020, 4:14:45 PM",
      "status": "FINISHED"
    },
    {
      "title": "Conditional update without overwrite",
      "text": "%spark\n\nimport io.delta.tables._\nimport org.apache.spark.sql.functions._\n\nval deltaTable = DeltaTable.forPath(\"/tmp/delta-table\")\n\n// Update every even value by adding 100 to it\ndeltaTable.update(\n  condition = expr(\"id % 2 == 0\"),\n  set = Map(\"id\" -> expr(\"id + 100\")))\n\n// Delete every even value\ndeltaTable.delete(condition = expr(\"id % 2 == 0\"))\n\n// Upsert (merge) new data\nval newData = spark.range(0, 20).toDF\n\ndeltaTable.as(\"oldData\")\n  .merge(\n    newData.as(\"newData\"),\n    \"oldData.id = newData.id\")\n  .whenMatched\n  .update(Map(\"id\" -> col(\"newData.id\")))\n  .whenNotMatched\n  .insert(Map(\"id\" -> col(\"newData.id\")))\n  .execute()\n\ndeltaTable.toDF.show()",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2020, 4:15:33 PM",
      "progress": 0,
      "config": {
        "colWidth": 6.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+---+\n| id|\n+---+\n| 15|\n| 16|\n|  1|\n| 18|\n| 14|\n|  4|\n|  8|\n| 17|\n|  0|\n| 10|\n|  6|\n|  2|\n|  3|\n| 13|\n|  5|\n| 12|\n| 19|\n|  7|\n|  9|\n| 11|\n+---+\n\nimport io.delta.tables._\nimport org.apache.spark.sql.functions._\n\u001b[1m\u001b[34mdeltaTable\u001b[0m: \u001b[1m\u001b[32mio.delta.tables.DeltaTable\u001b[0m = io.delta.tables.DeltaTable@355329ee\n\u001b[1m\u001b[34mnewData\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [id: bigint]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588147954117_626957150",
      "id": "paragraph_1588147954117_626957150",
      "dateCreated": "Apr 29, 2020, 4:12:34 PM",
      "dateStarted": "Apr 29, 2020, 4:15:33 PM",
      "dateFinished": "Apr 29, 2020, 4:15:48 PM",
      "status": "FINISHED"
    },
    {
      "title": "Read older versions of data using time travel",
      "text": "%spark\n\nval df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/tmp/delta-table\")\ndf.show()",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2020, 4:16:04 PM",
      "progress": 0,
      "config": {
        "colWidth": 6.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+---+\n| id|\n+---+\n|  0|\n|  3|\n|  1|\n|  2|\n|  4|\n+---+\n\n\u001b[1m\u001b[34mdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [id: bigint]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588148133131_1770029903",
      "id": "paragraph_1588148133131_1770029903",
      "dateCreated": "Apr 29, 2020, 4:15:33 PM",
      "dateStarted": "Apr 29, 2020, 4:16:04 PM",
      "dateFinished": "Apr 29, 2020, 4:16:08 PM",
      "status": "FINISHED"
    },
    {
      "text": "%spark\n",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2020, 4:18:21 PM",
      "progress": 0,
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588148301603_1997345504",
      "id": "paragraph_1588148301603_1997345504",
      "dateCreated": "Apr 29, 2020, 4:18:21 PM",
      "status": "READY"
    }
  ],
  "name": "7. Spark Delta Lake Tutorial",
  "id": "2F8VDBMMT",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0-SNAPSHOT",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {},
  "path": "/Spark Tutorial/7. Spark Delta Lake Tutorial"
}