{
  "paragraphs": [
    {
      "title": "Introduction",
      "text": "%md\n\nThis is a tutorial for Spark SQL in PySpark (based on Spark 2.x).  First we need to clarify several concepts of Spark SQL\n\n* **SparkSession**   - This is the entry point of Spark SQL, you need use `SparkSession` to create DataFrame/Dataset, register UDF, query table and etc.\n* **DataFrame**      - There's no Dataset in PySpark, but only DataFrame. The DataFrame of PySpark is very similar with DataFrame concept of Pandas, but is distributed. \n",
      "user": "anonymous",
      "dateUpdated": "Jul 26, 2021, 4:33:49 AM",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/text",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true,
        "runOnSelectionChange": true,
        "title": true,
        "checkEmpty": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>This is a tutorial for Spark SQL in PySpark (based on Spark 2.x).  First we need to clarify several concepts of Spark SQL</p>\n<ul>\n<li><strong>SparkSession</strong>   - This is the entry point of Spark SQL, you need use <code>SparkSession</code> to create DataFrame/Dataset, register UDF, query table and etc.</li>\n<li><strong>DataFrame</strong>      - There&rsquo;s no Dataset in PySpark, but only DataFrame. The DataFrame of PySpark is very similar with DataFrame concept of Pandas, but is distributed.</li>\n</ul>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1578387700849_2035279674",
      "id": "20180530-101118_380906698",
      "dateCreated": "Jan 7, 2020, 5:01:40 PM",
      "dateStarted": "Jul 26, 2021, 4:33:49 AM",
      "dateFinished": "Jul 26, 2021, 4:33:49 AM",
      "status": "FINISHED"
    },
    {
      "title": "Create DataFrame",
      "text": "%md\n\nThere're 2 ways to create DataFrame\n\n* Use SparkSession to create DataFrame directly. You can either create DataFrame from RDD, List type objects and etc.\n* Use DataFrameReader to create Dataset/DataFrame from many kinds of storages that are supported by spark, such as HDFS, jdbc and etc.",
      "user": "anonymous",
      "dateUpdated": "Jul 26, 2021, 4:34:40 AM",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true,
        "runOnSelectionChange": true,
        "checkEmpty": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>There&rsquo;re 2 ways to create DataFrame</p>\n<ul>\n<li>Use SparkSession to create DataFrame directly. You can either create DataFrame from RDD, List type objects and etc.</li>\n<li>Use DataFrameReader to create Dataset/DataFrame from many kinds of storages that are supported by spark, such as HDFS, jdbc and etc.</li>\n</ul>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1578387700850_-1934281275",
      "id": "20180530-101515_948520659",
      "dateCreated": "Jan 7, 2020, 5:01:40 PM",
      "dateStarted": "Jul 26, 2021, 4:34:40 AM",
      "dateFinished": "Jul 26, 2021, 4:34:40 AM",
      "status": "FINISHED"
    },
    {
      "title": "Prerequisites",
      "text": "%md\n\n\n\n**It is strongly recommended to run the following %spark.conf paragraph first to make sure correct configuration is used.**",
      "user": "anonymous",
      "dateUpdated": "Jul 26, 2021, 4:33:50 AM",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "title": false,
        "results": {},
        "enabled": true,
        "runOnSelectionChange": true,
        "checkEmpty": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p><strong>It is strongly recommended to run the following %spark.conf paragraph first to make sure correct configuration is used.</strong></p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1578387700850_-1709355244",
      "id": "20180530-110023_1756702033",
      "dateCreated": "Jan 7, 2020, 5:01:40 PM",
      "dateStarted": "Jul 26, 2021, 4:33:50 AM",
      "dateFinished": "Jul 26, 2021, 4:33:50 AM",
      "status": "FINISHED"
    },
    {
      "title": "Spark Configuration",
      "text": "%spark.conf\n\n# It is strongly recommended to set SPARK_HOME explicitly instead of using the embedded spark of Zeppelin. As the function of embedded spark of Zeppelin is limited and can only run in local mode.\n# SPARK_HOME <your_spark_dist_path>\n\n# Uncomment the following line if you want to use yarn-cluster mode (It is recommended to use yarn-cluster mode from Zeppelin 0.8, as the driver will run on the remote host of yarn cluster which can mitigate memory pressure of zeppelin server)\n# master yarn-cluster\n\n# Uncomment the following line if you want to use yarn-client mode (It is not recommended to use yarn-client after 0.8. Because it would launch the driver in the same host of zeppelin server which will increase memory pressure of zeppelin server)\n# master yarn-client\n\n# Uncomment the following line to enable HiveContext, and also put hive-site.xml under SPARK_CONF_DIR\n# zeppelin.spark.useHiveContext true\n",
      "user": "anonymous",
      "dateUpdated": "Jul 26, 2021, 4:33:50 AM",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/text",
        "fontSize": 9.0,
        "results": {},
        "enabled": true,
        "runOnSelectionChange": true,
        "title": true,
        "checkEmpty": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1578387700850_-1532509261",
      "id": "20180530-110007_162886838",
      "dateCreated": "Jan 7, 2020, 5:01:40 PM",
      "dateStarted": "Jul 26, 2021, 4:33:50 AM",
      "dateFinished": "Jul 26, 2021, 4:33:50 AM",
      "status": "FINISHED"
    },
    {
      "title": "Create Dataset/DataFrame via SparkSession",
      "text": "%spark.pyspark\n\n# create DataFrame from python list. It can infer schema for you.\ndf1 = spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]).toDF(\"id\", \"name\", \"age\", \"country\")\ndf1.printSchema()\ndf1.show()\n\n# create DataFrame from pandas dataframe\ndf2 = spark.createDataFrame(df1.toPandas())\ndf2.printSchema()\ndf2.show()\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "user": "anonymous",
      "dateUpdated": "Jul 26, 2021, 4:33:50 AM",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true,
        "runOnSelectionChange": true,
        "checkEmpty": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\n |-- country: string (nullable = true)\n\n+---+-----+---+-------+\n| id| name|age|country|\n+---+-----+---+-------+\n|  1| andy| 20|    USA|\n|  2| jeff| 23|  China|\n|  3|james| 18|    USA|\n+---+-----+---+-------+\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\n |-- country: string (nullable = true)\n\n+---+-----+---+-------+\n| id| name|age|country|\n+---+-----+---+-------+\n|  1| andy| 20|    USA|\n|  2| jeff| 23|  China|\n|  3|james| 18|    USA|\n+---+-----+---+-------+\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=61"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=62"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=63"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=64"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=65"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1578387700850_1345292725",
      "id": "20180530-101750_1491737301",
      "dateCreated": "Jan 7, 2020, 5:01:40 PM",
      "dateStarted": "Jul 26, 2021, 4:33:50 AM",
      "dateFinished": "Jul 26, 2021, 4:33:51 AM",
      "status": "FINISHED"
    },
    {
      "title": "Create DataFrame via DataFrameReader",
      "text": "%spark.pyspark\n\nSPARK_HOME = os.getenv('SPARK_HOME')\n\n# Read data from json file\n# link for this people.json (https://github.com/apache/spark/blob/master/examples/src/main/resources/people.json)\n# Use hdfs path if you are using hdfs\ndf1 = spark.read.json(\"file://\" + SPARK_HOME + \"/examples/src/main/resources/people.json\")\ndf1.printSchema()\ndf1.show()\n\n# Read data from csv file. You can customize it via spark.read.options. E.g. In the following example, we customize the sep and header\ndf2 = spark.read.options(sep=\";\", header=True).csv(\"file://\"  + SPARK_HOME + \"/examples/src/main/resources/people.csv\")\ndf2.printSchema()\ndf2.show()\n\n# Specify schema for your csv file\nfrom pyspark.sql.types import StructType, StringType, IntegerType\n\nschema = StructType().add(\"name\", StringType(), True) \\\n    .add(\"age\", IntegerType(), True) \\\n    .add(\"job\", StringType(), True)\n    \ndf3 = spark.read.options(sep=\";\", header=True) \\\n    .schema(schema) \\\n    .csv(\"file://\" + SPARK_HOME + \"/examples/src/main/resources/people.csv\") \ndf3.printSchema()\ndf3.show()\n\n",
      "user": "anonymous",
      "dateUpdated": "Jul 26, 2021, 4:33:51 AM",
      "progress": 100,
      "config": {
        "lineNumbers": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true,
        "runOnSelectionChange": true,
        "checkEmpty": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "root\n |-- age: long (nullable = true)\n |-- name: string (nullable = true)\n\n+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+\n\nroot\n |-- name: string (nullable = true)\n |-- age: string (nullable = true)\n |-- job: string (nullable = true)\n\n+-----+---+---------+\n| name|age|      job|\n+-----+---+---------+\n|Jorge| 30|Developer|\n|  Bob| 32|Developer|\n+-----+---+---------+\n\nroot\n |-- name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- job: string (nullable = true)\n\n+-----+---+---------+\n| name|age|      job|\n+-----+---+---------+\n|Jorge| 30|Developer|\n|  Bob| 32|Developer|\n+-----+---+---------+\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=66"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=67"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=68"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=69"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=70"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1578387700850_581443636",
      "id": "20180530-101930_1495479697",
      "dateCreated": "Jan 7, 2020, 5:01:40 PM",
      "dateStarted": "Jul 26, 2021, 4:33:51 AM",
      "dateFinished": "Jul 26, 2021, 4:33:52 AM",
      "status": "FINISHED"
    },
    {
      "title": "Add New Column",
      "text": "%spark.pyspark\n\n# withColumn could be used to add new Column\ndf1 = spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]).toDF(\"id\", \"name\", \"age\", \"country\")\n\ndf2 = df1.withColumn(\"age2\", df1[\"age\"] + 1)\ndf2.show()\n\n# the new column could replace the existing the column if the new column name is the same as the old column\ndf3 = df1.withColumn(\"age\", df1[\"age\"] + 1)\ndf3.show()\n\n# Besides using expression to create new column, you could also use udf to create new column\n# Use F.upper instead of upper, because the builtin udf of spark may conclifct with that of python, such as max\nimport pyspark.sql.functions as F\ndf4 = df1.withColumn(\"name\", F.upper(df1[\"name\"]))\ndf4.show()",
      "user": "anonymous",
      "dateUpdated": "Jul 26, 2021, 4:33:52 AM",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true,
        "runOnSelectionChange": true,
        "checkEmpty": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+---+-----+---+-------+----+\n| id| name|age|country|age2|\n+---+-----+---+-------+----+\n|  1| andy| 20|    USA|  21|\n|  2| jeff| 23|  China|  24|\n|  3|james| 18|    USA|  19|\n+---+-----+---+-------+----+\n\n+---+-----+---+-------+\n| id| name|age|country|\n+---+-----+---+-------+\n|  1| andy| 21|    USA|\n|  2| jeff| 24|  China|\n|  3|james| 19|    USA|\n+---+-----+---+-------+\n\n+---+-----+---+-------+\n| id| name|age|country|\n+---+-----+---+-------+\n|  1| ANDY| 20|    USA|\n|  2| JEFF| 23|  China|\n|  3|JAMES| 18|    USA|\n+---+-----+---+-------+\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=71"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=72"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=73"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=74"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=75"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=76"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1578387700850_-775755394",
      "id": "20180530-105113_693855403",
      "dateCreated": "Jan 7, 2020, 5:01:40 PM",
      "dateStarted": "Jul 26, 2021, 4:33:52 AM",
      "dateFinished": "Jul 26, 2021, 4:33:52 AM",
      "status": "FINISHED"
    },
    {
      "title": "Remove Column",
      "text": "%spark.pyspark\n\n\ndf1 = spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]).toDF(\"id\", \"name\", \"age\", \"country\")\n# drop could be used to remove Column\ndf2 = df1.drop(\"id\")\ndf2.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "user": "anonymous",
      "dateUpdated": "Jul 26, 2021, 4:33:52 AM",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true,
        "runOnSelectionChange": true,
        "checkEmpty": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-----+---+-------+\n| name|age|country|\n+-----+---+-------+\n| andy| 20|    USA|\n| jeff| 23|  China|\n|james| 18|    USA|\n+-----+---+-------+\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=77"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=78"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1578387700850_-886487025",
      "id": "20180530-112045_1274721210",
      "dateCreated": "Jan 7, 2020, 5:01:40 PM",
      "dateStarted": "Jul 26, 2021, 4:33:52 AM",
      "dateFinished": "Jul 26, 2021, 4:33:53 AM",
      "status": "FINISHED"
    },
    {
      "title": "Select Subset of Columns",
      "text": "%spark.pyspark\n\ndf1 = spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]).toDF(\"id\", \"name\", \"age\", \"country\")\n# select can accept a list of string of the column names\ndf2 = df1.select(\"id\", \"name\")\ndf2.show()\n\n# select can also accept a list of Column. You can create column via $ or udf\nimport pyspark.sql.functions as F\ndf3 = df1.select(df1[\"id\"], F.upper(df1[\"name\"]), df1[\"age\"] + 1)\ndf3.show()\n\n\n",
      "user": "anonymous",
      "dateUpdated": "Jul 26, 2021, 4:33:53 AM",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true,
        "runOnSelectionChange": true,
        "checkEmpty": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+---+-----+\n| id| name|\n+---+-----+\n|  1| andy|\n|  2| jeff|\n|  3|james|\n+---+-----+\n\n+---+-----------+---------+\n| id|upper(name)|(age + 1)|\n+---+-----------+---------+\n|  1|       ANDY|       21|\n|  2|       JEFF|       24|\n|  3|      JAMES|       19|\n+---+-----------+---------+\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=79"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=80"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=81"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=82"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1578387700850_2124268380",
      "id": "20180530-113042_1154914545",
      "dateCreated": "Jan 7, 2020, 5:01:40 PM",
      "dateStarted": "Jul 26, 2021, 4:33:53 AM",
      "dateFinished": "Jul 26, 2021, 4:33:53 AM",
      "status": "FINISHED"
    },
    {
      "title": "Filter Rows",
      "text": "%spark.pyspark\n\ndf1 = spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]).toDF(\"id\", \"name\", \"age\", \"country\")\n\n# filter accept a Column \ndf2 = df1.filter(df1[\"age\"] >= 20)\ndf2.show()\n\n# To be noticed, you need to use \"&\" instead of \"&&\" or \"AND\" \ndf3 = df1.filter((df1[\"age\"] >= 20) & (df1[\"country\"] == \"China\"))\ndf3.show()\n\n\n\n\n\n\n",
      "user": "anonymous",
      "dateUpdated": "Jul 26, 2021, 4:33:53 AM",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true,
        "runOnSelectionChange": true,
        "checkEmpty": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+---+----+---+-------+\n| id|name|age|country|\n+---+----+---+-------+\n|  1|andy| 20|    USA|\n|  2|jeff| 23|  China|\n+---+----+---+-------+\n\n+---+----+---+-------+\n| id|name|age|country|\n+---+----+---+-------+\n|  2|jeff| 23|  China|\n+---+----+---+-------+\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=83"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=84"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=85"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=86"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1578387700850_1501705200",
      "id": "20180530-113407_58454283",
      "dateCreated": "Jan 7, 2020, 5:01:40 PM",
      "dateStarted": "Jul 26, 2021, 4:33:53 AM",
      "dateFinished": "Jul 26, 2021, 4:33:54 AM",
      "status": "FINISHED"
    },
    {
      "title": "Create UDF",
      "text": "%spark.pyspark\n\ndf1 = spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]) \\\n            .toDF(\"id\", \"name\", \"age\", \"country\")\n\n# Create udf create python lambda\nfrom pyspark.sql.functions import udf\nudf1 = udf(lambda e: e.upper())\ndf2 = df1.select(udf1(df1[\"name\"]))\ndf2.show()\n\n# UDF could also be used in filter, in this case the return type must be Boolean\n# We can also use annotation to create udf\nfrom pyspark.sql.types import *\n@udf(returnType=BooleanType())\ndef udf2(e):\n    if e >= 20:\n        return True;\n    else:\n        return False\n\ndf3 = df1.filter(udf2(df1[\"age\"]))\ndf3.show()\n\n# UDF could also accept more than 1 argument.\nudf3 = udf(lambda e1, e2: e1 + \"_\" + e2)\ndf4 = df1.select(udf3(df1[\"name\"], df1[\"country\"]).alias(\"name_country\"))\ndf4.show()\n",
      "user": "anonymous",
      "dateUpdated": "Jul 26, 2021, 4:33:54 AM",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true,
        "runOnSelectionChange": true,
        "checkEmpty": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+--------------+\n|<lambda>(name)|\n+--------------+\n|          ANDY|\n|          JEFF|\n|         JAMES|\n+--------------+\n\n+---+----+---+-------+\n| id|name|age|country|\n+---+----+---+-------+\n|  1|andy| 20|    USA|\n|  2|jeff| 23|  China|\n+---+----+---+-------+\n\n+------------+\n|name_country|\n+------------+\n|    andy_USA|\n|  jeff_China|\n|   james_USA|\n+------------+\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=87"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=88"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=89"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=90"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=91"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=92"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1578387700850_574730063",
      "id": "20180530-113720_1986531680",
      "dateCreated": "Jan 7, 2020, 5:01:40 PM",
      "dateStarted": "Jul 26, 2021, 4:33:54 AM",
      "dateFinished": "Jul 26, 2021, 4:33:55 AM",
      "status": "FINISHED"
    },
    {
      "title": "GroupBy",
      "text": "%spark.pyspark\n\ndf1 = spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]) \\\n           .toDF(\"id\", \"name\", \"age\", \"country\")\n\n# You can call agg function after groupBy directly, such as count/min/max/avg/sum\ndf2 = df1.groupBy(\"country\").count()\ndf2.show()\n\n# Pass a Map if you want to do multiple aggregation\ndf3 = df1.groupBy(\"country\").agg({\"age\": \"avg\", \"id\": \"count\"})\ndf3.show()\n\nimport pyspark.sql.functions as F\n# Or you can pass a list of agg function\ndf4 = df1.groupBy(\"country\").agg(F.avg(df1[\"age\"]).alias(\"avg_age\"), F.count(df1[\"id\"]).alias(\"count\"))\ndf4.show()\n\n# You can not pass Map if you want to do multiple aggregation on the same column as the key of Map should be unique. So in this case\n# you have to pass a list of agg functions\ndf5 = df1.groupBy(\"country\").agg(F.avg(df1[\"age\"]).alias(\"avg_age\"), F.max(df1[\"age\"]).alias(\"max_age\"))\ndf5.show()\n\n\n\n\n\n\n",
      "user": "anonymous",
      "dateUpdated": "Jul 26, 2021, 4:33:55 AM",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true,
        "runOnSelectionChange": true,
        "checkEmpty": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-------+-----+\n|country|count|\n+-------+-----+\n|  China|    1|\n|    USA|    2|\n+-------+-----+\n\n+-------+---------+--------+\n|country|count(id)|avg(age)|\n+-------+---------+--------+\n|  China|        1|    23.0|\n|    USA|        2|    19.0|\n+-------+---------+--------+\n\n+-------+-------+-----+\n|country|avg_age|count|\n+-------+-------+-----+\n|  China|   23.0|    1|\n|    USA|   19.0|    2|\n+-------+-------+-----+\n\n+-------+-------+-------+\n|country|avg_age|max_age|\n+-------+-------+-------+\n|  China|   23.0|     23|\n|    USA|   19.0|     20|\n+-------+-------+-------+\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=93"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=94"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=95"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=96"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=97"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=98"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=99"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=100"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=101"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=102"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=103"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=104"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=105"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=106"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=107"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=108"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=109"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=110"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=111"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=112"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1578387700850_1233271138",
      "id": "20180530-114404_2076888937",
      "dateCreated": "Jan 7, 2020, 5:01:40 PM",
      "dateStarted": "Jul 26, 2021, 4:33:55 AM",
      "dateFinished": "Jul 26, 2021, 4:33:59 AM",
      "status": "FINISHED"
    },
    {
      "title": "Join on Single Field",
      "text": "%spark.pyspark\n\ndf1 = spark.createDataFrame([(1, \"andy\", 20, 1), (2, \"jeff\", 23, 2), (3, \"james\", 18, 3)]).toDF(\"id\", \"name\", \"age\", \"c_id\")\ndf1.show()\n\ndf2 = spark.createDataFrame([(1, \"USA\"), (2, \"China\")]).toDF(\"c_id\", \"c_name\")\ndf2.show()\n\n# You can just specify the key name if join on the same key\ndf3 = df1.join(df2, \"c_id\")\ndf3.show()\n\n# Or you can specify the join condition expclitly in case the key is different between tables\ndf4 = df1.join(df2, df1[\"c_id\"] == df2[\"c_id\"])\ndf4.show()\n\n# You can specify the join type afte the join condition, by default it is inner join\ndf5 = df1.join(df2, df1[\"c_id\"] == df2[\"c_id\"], \"left_outer\")\ndf5.show()",
      "user": "anonymous",
      "dateUpdated": "Jul 26, 2021, 4:33:59 AM",
      "progress": 25,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true,
        "runOnSelectionChange": true,
        "checkEmpty": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+---+-----+---+----+\n| id| name|age|c_id|\n+---+-----+---+----+\n|  1| andy| 20|   1|\n|  2| jeff| 23|   2|\n|  3|james| 18|   3|\n+---+-----+---+----+\n\n+----+------+\n|c_id|c_name|\n+----+------+\n|   1|   USA|\n|   2| China|\n+----+------+\n\n+----+---+----+---+------+\n|c_id| id|name|age|c_name|\n+----+---+----+---+------+\n|   1|  1|andy| 20|   USA|\n|   2|  2|jeff| 23| China|\n+----+---+----+---+------+\n\n+---+----+---+----+----+------+\n| id|name|age|c_id|c_id|c_name|\n+---+----+---+----+----+------+\n|  1|andy| 20|   1|   1|   USA|\n|  2|jeff| 23|   2|   2| China|\n+---+----+---+----+----+------+\n\n+---+-----+---+----+----+------+\n| id| name|age|c_id|c_id|c_name|\n+---+-----+---+----+----+------+\n|  1| andy| 20|   1|   1|   USA|\n|  3|james| 18|   3|null|  null|\n|  2| jeff| 23|   2|   2| China|\n+---+-----+---+----+----+------+\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=113"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=114"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=115"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=116"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=117"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=118"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=119"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=120"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=121"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=122"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=123"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=124"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=125"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=126"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=127"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=128"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=129"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=130"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=131"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1578387700851_-770209064",
      "id": "20180530-130126_1642948432",
      "dateCreated": "Jan 7, 2020, 5:01:40 PM",
      "dateStarted": "Jul 26, 2021, 4:33:59 AM",
      "dateFinished": "Jul 26, 2021, 4:34:03 AM",
      "status": "FINISHED"
    },
    {
      "title": "Join on Multiple Fields",
      "text": "%spark.pyspark\n\ndf1 = spark.createDataFrame([(\"andy\", 20, 1, 1), (\"jeff\", 23, 1, 2), (\"james\", 12, 2, 2)]).toDF(\"name\", \"age\", \"key_1\", \"key_2\")\ndf1.show()\n\ndf2 = spark.createDataFrame([(1, 1, \"USA\"), (2, 2, \"China\")]).toDF(\"key_1\", \"key_2\", \"country\")\ndf2.show()\n\n# Join on 2 fields: key_1, key_2\n\n# You can pass a list of field name if the join field names are the same in both tables\ndf3 = df1.join(df2, [\"key_1\", \"key_2\"])\ndf3.show()\n\n# Or you can specify the join condition expclitly in case when the join fields name is differetnt in the two tables\ndf4 = df1.join(df2, (df1[\"key_1\"] == df2[\"key_1\"]) & (df1[\"key_2\"] == df2[\"key_2\"]))\ndf4.show()\n\n",
      "user": "anonymous",
      "dateUpdated": "Jul 26, 2021, 4:34:03 AM",
      "progress": 27,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true,
        "runOnSelectionChange": true,
        "checkEmpty": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-----+---+-----+-----+\n| name|age|key_1|key_2|\n+-----+---+-----+-----+\n| andy| 20|    1|    1|\n| jeff| 23|    1|    2|\n|james| 12|    2|    2|\n+-----+---+-----+-----+\n\n+-----+-----+-------+\n|key_1|key_2|country|\n+-----+-----+-------+\n|    1|    1|    USA|\n|    2|    2|  China|\n+-----+-----+-------+\n\n+-----+-----+-----+---+-------+\n|key_1|key_2| name|age|country|\n+-----+-----+-----+---+-------+\n|    1|    1| andy| 20|    USA|\n|    2|    2|james| 12|  China|\n+-----+-----+-----+---+-------+\n\n+-----+---+-----+-----+-----+-----+-------+\n| name|age|key_1|key_2|key_1|key_2|country|\n+-----+---+-----+-----+-----+-----+-------+\n| andy| 20|    1|    1|    1|    1|    USA|\n|james| 12|    2|    2|    2|    2|  China|\n+-----+---+-----+-----+-----+-----+-------+\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=132"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=133"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=134"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=135"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=136"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=137"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=138"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=139"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=140"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=141"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=142"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=143"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=144"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=145"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1578387700851_-177297320",
      "id": "20180530-135600_354945835",
      "dateCreated": "Jan 7, 2020, 5:01:40 PM",
      "dateStarted": "Jul 26, 2021, 4:34:03 AM",
      "dateFinished": "Jul 26, 2021, 4:34:05 AM",
      "status": "FINISHED"
    },
    {
      "title": "Use SQL directly",
      "text": "%spark.pyspark\n\ndf1 = spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]) \\\n           .toDF(\"id\", \"name\", \"age\", \"country\")\n# call createOrReplaceTempView first if you want to query this DataFrame via sql\ndf1.createOrReplaceTempView(\"people\")\n# SparkSession.sql return DataFrame\ndf2 = spark.sql(\"select name, age from people\")\ndf2.show()\n\n# You need to register udf if you want to use it in sql\nspark.udf.register(\"udf1\", lambda e : e.upper())\ndf3 = spark.sql(\"select udf1(name), age from people\")\ndf3.show()",
      "user": "anonymous",
      "dateUpdated": "Jul 26, 2021, 4:34:05 AM",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true,
        "runOnSelectionChange": true,
        "checkEmpty": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-----+---+\n| name|age|\n+-----+---+\n| andy| 20|\n| jeff| 23|\n|james| 18|\n+-----+---+\n\n+----------+---+\n|udf1(name)|age|\n+----------+---+\n|      ANDY| 20|\n|      JEFF| 23|\n|     JAMES| 18|\n+----------+---+\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=146"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=147"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=148"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=149"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1578387700851_1756979054",
      "id": "20180530-132023_995737505",
      "dateCreated": "Jan 7, 2020, 5:01:40 PM",
      "dateStarted": "Jul 26, 2021, 4:34:05 AM",
      "dateFinished": "Jul 26, 2021, 4:34:06 AM",
      "status": "FINISHED"
    },
    {
      "title": "Show Tables",
      "text": "%spark.sql\n\nshow tables",
      "user": "anonymous",
      "dateUpdated": "Jul 26, 2021, 4:34:06 AM",
      "progress": 0,
      "config": {
        "runOnSelectionChange": true,
        "title": true,
        "checkEmpty": true,
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "database": "string",
                      "tableName": "string",
                      "isTemporary": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/sql"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "database\ttableName\tisTemporary\n\tbank\ttrue\n\tpeople\ttrue\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1578388432752_877455036",
      "id": "paragraph_1578388432752_877455036",
      "dateCreated": "Jan 7, 2020, 5:13:52 PM",
      "dateStarted": "Jul 26, 2021, 4:34:06 AM",
      "dateFinished": "Jul 26, 2021, 4:34:06 AM",
      "status": "FINISHED"
    },
    {
      "title": "Visualize DataFrame/Dataset",
      "text": "%md\n\nThere's 2 approaches to visualize DataFrame/Dataset in Zeppelin\n\n* Use SparkSQLInterpreter via `%spark.sql`\n* Use ZeppelinContext via `z.show`\n\n",
      "user": "anonymous",
      "dateUpdated": "Jul 26, 2021, 4:34:06 AM",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "title": false,
        "results": {},
        "enabled": true,
        "runOnSelectionChange": true,
        "checkEmpty": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>There&rsquo;s 2 approaches to visualize DataFrame/Dataset in Zeppelin</p>\n<ul>\n<li>Use SparkSQLInterpreter via <code>%spark.sql</code></li>\n<li>Use ZeppelinContext via <code>z.show</code></li>\n</ul>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1578387700851_1924561483",
      "id": "20180530-132128_2114955642",
      "dateCreated": "Jan 7, 2020, 5:01:40 PM",
      "dateStarted": "Jul 26, 2021, 4:34:06 AM",
      "dateFinished": "Jul 26, 2021, 4:34:06 AM",
      "status": "FINISHED"
    },
    {
      "title": "Visualize DataFrame/Dataset via z.show",
      "text": "%spark.pyspark\n\ndf1 = spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]).toDF(\"id\", \"name\", \"age\", \"country\")\ndf2 = df1.groupBy(\"country\").count()\nz.show(df2)\n\n\n\n\n\n\n",
      "user": "anonymous",
      "dateUpdated": "Jul 26, 2021, 4:34:06 AM",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "title": true,
        "results": {
          "0": {
            "graph": {
              "mode": "multiBarChart",
              "height": 300.0,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "country": "string",
                      "count": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                },
                "multiBarChart": {
                  "rotate": {
                    "degree": "-45"
                  },
                  "xLabelStatus": "default"
                }
              },
              "commonSetting": {},
              "keys": [
                {
                  "name": "country",
                  "index": 0.0,
                  "aggr": "sum"
                }
              ],
              "groups": [],
              "values": [
                {
                  "name": "count",
                  "index": 1.0,
                  "aggr": "sum"
                }
              ]
            },
            "helium": {}
          },
          "1": {
            "graph": {
              "mode": "multiBarChart",
              "height": 300.0,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "country": "string",
                      "count": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                },
                "multiBarChart": {
                  "rotate": {
                    "degree": "-45"
                  },
                  "xLabelStatus": "default"
                }
              },
              "commonSetting": {},
              "keys": [
                {
                  "name": "country",
                  "index": 0.0,
                  "aggr": "sum"
                }
              ],
              "groups": [],
              "values": [
                {
                  "name": "count",
                  "index": 1.0,
                  "aggr": "sum"
                }
              ]
            },
            "helium": {}
          }
        },
        "enabled": true,
        "runOnSelectionChange": true,
        "checkEmpty": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "country\tcount\nChina\t1\nUSA\t2\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=150"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=151"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=152"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=153"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=154"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1578387700851_-809695439",
      "id": "20180530-132634_1285621466",
      "dateCreated": "Jan 7, 2020, 5:01:40 PM",
      "dateStarted": "Jul 26, 2021, 4:34:06 AM",
      "dateFinished": "Jul 26, 2021, 4:34:07 AM",
      "status": "FINISHED"
    },
    {
      "title": "Visualize DataFrame/Dataset via %spark.sql",
      "text": "%spark.pyspark\n\ndf1 = spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]) \\\n            .toDF(\"id\", \"name\", \"age\", \"country\")\n            \n# register this DataFrame first before querying it via %spark.sql\ndf1.createOrReplaceTempView(\"people\")",
      "user": "anonymous",
      "dateUpdated": "Jul 26, 2021, 4:34:07 AM",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true,
        "runOnSelectionChange": true,
        "checkEmpty": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1578387700851_381269276",
      "id": "20180530-132657_668624333",
      "dateCreated": "Jan 7, 2020, 5:01:40 PM",
      "dateStarted": "Jul 26, 2021, 4:34:07 AM",
      "dateFinished": "Jul 26, 2021, 4:34:07 AM",
      "status": "FINISHED"
    },
    {
      "title": "",
      "text": "%spark.sql\n\nselect country, count(1) as count from people group by country",
      "user": "anonymous",
      "dateUpdated": "Jul 26, 2021, 4:34:08 AM",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {
          "0": {
            "graph": {
              "mode": "multiBarChart",
              "height": 300.0,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {
                    "columns": [
                      {
                        "name": "country",
                        "visible": true,
                        "width": "*",
                        "sort": {},
                        "filters": [
                          {}
                        ],
                        "pinned": ""
                      },
                      {
                        "name": "count",
                        "visible": true,
                        "width": "*",
                        "sort": {},
                        "filters": [
                          {}
                        ],
                        "pinned": ""
                      }
                    ],
                    "scrollFocus": {},
                    "selection": [],
                    "grouping": {
                      "grouping": [],
                      "aggregations": [],
                      "rowExpandedStates": {}
                    },
                    "treeView": {},
                    "pagination": {
                      "paginationCurrentPage": 1.0,
                      "paginationPageSize": 250.0
                    }
                  },
                  "tableColumnTypeState": {
                    "names": {
                      "country": "string",
                      "count": "number"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                },
                "multiBarChart": {
                  "rotate": {
                    "degree": "-45"
                  },
                  "xLabelStatus": "default",
                  "stacked": false
                },
                "stackedAreaChart": {
                  "rotate": {
                    "degree": "-45"
                  },
                  "xLabelStatus": "default"
                },
                "lineChart": {
                  "rotate": {
                    "degree": "-45"
                  },
                  "xLabelStatus": "default"
                }
              },
              "commonSetting": {},
              "keys": [
                {
                  "name": "country",
                  "index": 0.0,
                  "aggr": "sum"
                }
              ],
              "groups": [],
              "values": [
                {
                  "name": "count",
                  "index": 1.0,
                  "aggr": "sum"
                }
              ]
            },
            "helium": {}
          }
        },
        "enabled": true,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "country\tcount\nChina\t1\nUSA\t2\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=155"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=156"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=157"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=158"
            },
            {
              "jobUrl": "http://3745c7ed824d:4040/jobs/job?id=159"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1578387700851_601200360",
      "id": "20180530-132823_944494152",
      "dateCreated": "Jan 7, 2020, 5:01:40 PM",
      "dateStarted": "Jul 26, 2021, 4:34:08 AM",
      "dateFinished": "Jul 26, 2021, 4:34:08 AM",
      "status": "FINISHED"
    },
    {
      "title": "",
      "text": "%spark.sql\n",
      "user": "anonymous",
      "dateUpdated": "Jul 26, 2021, 4:34:08 AM",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1578387700851_-2136199261",
      "id": "20180530-132849_1305166760",
      "dateCreated": "Jan 7, 2020, 5:01:40 PM",
      "dateStarted": "Jul 26, 2021, 4:34:08 AM",
      "dateFinished": "Jul 26, 2021, 4:34:08 AM",
      "status": "FINISHED"
    }
  ],
  "name": "3. Spark SQL (PySpark)",
  "id": "2EWM84JXA",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0-SNAPSHOT",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {},
  "path": "/Spark Tutorial/3. Spark SQL (PySpark)"
}